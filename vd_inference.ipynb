{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio\n",
    "# !pip install transformers\n",
    "# !pip install einops\n",
    "# !pip install Pillow==9.1.0\n",
    "import gradio as gr\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from contextlib import nullcontext\n",
    "import types\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as tvtrans\n",
    "from lib.cfg_helper import model_cfg_bank\n",
    "from lib.model_zoo import get_model\n",
    "# from cusomized_gradio_blocks import create_myexamples, customized_as_example, customized_postprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_image = 2\n",
    "n_sample_text = 4\n",
    "cache_examples = True\n",
    "\n",
    "from lib.model_zoo.ddim import DDIMSampler\n",
    "\n",
    "def highlight_print(info):\n",
    "    print('')\n",
    "    print(''.join(['#']*(len(info)+4)))\n",
    "    print('# '+info+' #')\n",
    "    print(''.join(['#']*(len(info)+4)))\n",
    "    print('')\n",
    "\n",
    "def decompose(x, q=20, niter=100):\n",
    "    x_mean = x.mean(-1, keepdim=True)\n",
    "    x_input = x - x_mean\n",
    "    u, s, v = torch.pca_lowrank(x_input, q=q, center=False, niter=niter)\n",
    "    ss = torch.stack([torch.diag(si) for si in s])\n",
    "    x_lowrank = torch.bmm(torch.bmm(u, ss), torch.permute(v, [0, 2, 1]))\n",
    "    x_remain = x_input - x_lowrank\n",
    "    return u, s, v, x_mean, x_remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adjust_rank(object):\n",
    "    def __init__(self, max_drop_rank=[1, 5], q=20):\n",
    "        self.max_semantic_drop_rank = max_drop_rank[0]\n",
    "        self.max_style_drop_rank = max_drop_rank[1]\n",
    "        self.q = q\n",
    "\n",
    "        def t2y0_semf_wrapper(t0, y00, t1, y01):\n",
    "            return lambda t: (np.exp((t-0.5)*2)-t0)/(t1-t0)*(y01-y00)+y00\n",
    "        t0, y00 = np.exp((0  -0.5)*2), -self.max_semantic_drop_rank\n",
    "        t1, y01 = np.exp((0.5-0.5)*2), 1\n",
    "        self.t2y0_semf = t2y0_semf_wrapper(t0, y00, t1, y01)\n",
    "\n",
    "        def x2y_semf_wrapper(x0, x1, y1):\n",
    "            return lambda x, y0: (x-x0)/(x1-x0)*(y1-y0)+y0\n",
    "        x0 = 0\n",
    "        x1, y1 = self.max_semantic_drop_rank+1, 1\n",
    "        self.x2y_semf = x2y_semf_wrapper(x0, x1, y1)\n",
    "        \n",
    "        def t2y0_styf_wrapper(t0, y00, t1, y01):\n",
    "            return lambda t: (np.exp((t-0.5)*2)-t0)/(t1-t0)*(y01-y00)+y00\n",
    "        t0, y00 = np.exp((1  -0.5)*2), -(q-self.max_style_drop_rank)\n",
    "        t1, y01 = np.exp((0.5-0.5)*2), 1\n",
    "        self.t2y0_styf = t2y0_styf_wrapper(t0, y00, t1, y01)\n",
    "\n",
    "        def x2y_styf_wrapper(x0, x1, y1):\n",
    "            return lambda x, y0: (x-x0)/(x1-x0)*(y1-y0)+y0\n",
    "        x0 = q-1\n",
    "        x1, y1 = self.max_style_drop_rank-1, 1\n",
    "        self.x2y_styf = x2y_styf_wrapper(x0, x1, y1)\n",
    "\n",
    "    def __call__(self, x, lvl):\n",
    "        if lvl == 0.5:\n",
    "            return x\n",
    "\n",
    "        if x.dtype == torch.float16:\n",
    "            fp16 = True\n",
    "            x = x.float()\n",
    "        else:\n",
    "            fp16 = False\n",
    "        std_save = x.std(axis=[-2, -1])\n",
    "\n",
    "        u, s, v, x_mean, x_remain = decompose(x, q=self.q)\n",
    "\n",
    "        if lvl < 0.5:\n",
    "            assert lvl>=0\n",
    "            for xi in range(0, self.max_semantic_drop_rank+1):\n",
    "                y0 = self.t2y0_semf(lvl)\n",
    "                yi = self.x2y_semf(xi, y0)\n",
    "                yi = 0 if yi<0 else yi\n",
    "                s[:, xi] *= yi\n",
    "\n",
    "        elif lvl > 0.5:\n",
    "            assert lvl <= 1\n",
    "            for xi in range(self.max_style_drop_rank, self.q):\n",
    "                y0 = self.t2y0_styf(lvl)\n",
    "                yi = self.x2y_styf(xi, y0)\n",
    "                yi = 0 if yi<0 else yi\n",
    "                s[:, xi] *= yi\n",
    "            x_remain = 0\n",
    "\n",
    "        ss = torch.stack([torch.diag(si) for si in s])\n",
    "        x_lowrank = torch.bmm(torch.bmm(u, ss), torch.permute(v, [0, 2, 1]))\n",
    "        x_new = x_lowrank + x_mean + x_remain\n",
    "\n",
    "        std_new = x_new.std(axis=[-2, -1])\n",
    "        x_new = x_new / std_new * std_save\n",
    "\n",
    "        if fp16:\n",
    "            x_new = x_new.half()\n",
    "\n",
    "        return x_new\n",
    "def remove_duplicate_word(tx):\n",
    "    def combine_words(input, length):\n",
    "        combined_inputs = []\n",
    "        if len(splitted_input)>1:\n",
    "            for i in range(len(input)-1):\n",
    "                combined_inputs.append(input[i]+\" \"+last_word_of(splitted_input[i+1],length)) #add the last word of the right-neighbour (overlapping) sequence (before it has expanded), which is the next word in the original sentence\n",
    "        return combined_inputs, length+1\n",
    "\n",
    "    def remove_duplicates(input, length):\n",
    "        bool_broke=False #this means we didn't find any duplicates here\n",
    "        for i in range(len(input) - length):\n",
    "            if input[i]==input[i + length]: #found a duplicate piece of sentence!\n",
    "                for j in range(0, length): #remove the overlapping sequences in reverse order\n",
    "                    del input[i + length - j]\n",
    "                bool_broke = True\n",
    "                break #break the for loop as the loop length does not matches the length of splitted_input anymore as we removed elements\n",
    "        if bool_broke:\n",
    "            return remove_duplicates(input, length) #if we found a duplicate, look for another duplicate of the same length\n",
    "        return input\n",
    "\n",
    "    def last_word_of(input, length):\n",
    "        splitted = input.split(\" \")\n",
    "        if len(splitted)==0:\n",
    "            return input\n",
    "        else:\n",
    "            return splitted[length-1]\n",
    "\n",
    "    def split_and_puncsplit(text):\n",
    "        tx = text.split(\" \")\n",
    "        txnew = []\n",
    "        for txi in tx:\n",
    "            txqueue=[]\n",
    "            while True:\n",
    "                if txi[0] in '([{':\n",
    "                    txqueue.extend([txi[:1], '<puncnext>'])\n",
    "                    txi = txi[1:]\n",
    "                    if len(txi) == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            txnew += txqueue\n",
    "            txstack=[]\n",
    "            if len(txi) == 0:\n",
    "                continue\n",
    "            while True:\n",
    "                if txi[-1] in '?!.,:;}])':\n",
    "                    txstack = ['<puncnext>', txi[-1:]] + txstack\n",
    "                    txi = txi[:-1]\n",
    "                    if len(txi) == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            if len(txi) != 0:\n",
    "                txnew += [txi]\n",
    "            txnew += txstack\n",
    "        return txnew\n",
    "\n",
    "    if tx == '':\n",
    "        return tx\n",
    "\n",
    "    splitted_input = split_and_puncsplit(tx)\n",
    "    word_length = 1\n",
    "    intermediate_output = False\n",
    "    while len(splitted_input)>1:\n",
    "        splitted_input = remove_duplicates(splitted_input, word_length)\n",
    "        if len(splitted_input)>1:\n",
    "            splitted_input, word_length = combine_words(splitted_input, word_length)\n",
    "        if intermediate_output:\n",
    "            print(splitted_input)\n",
    "            print(word_length)\n",
    "    output = splitted_input[0]\n",
    "    output = output.replace(' <puncnext> ', '')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########\n",
      "# v1.0 #\n",
      "########\n",
      "\n",
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Load pth from pretrained/kl-f8.pth\n",
      "Load autoencoderkl with total 83653863 parameters,72921.759 parameter sum.\n",
      "Load optimus_bert_connector with total 109489920 parameters,19329.128 parameter sum.\n",
      "Load optimus_gpt2_connector with total 132109824 parameters,19388.662 parameter sum.\n",
      "Load pth from pretrained/optimus-vae.pth\n",
      "Load optimus_vae_next with total 241599744 parameters,-344611.688 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load clip_image_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load clip_text_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load openai_unet_2d_next with total 859520964 parameters,99720.427 parameter sum.\n",
      "Load openai_unet_0d_next with total 1706797888 parameters,250144.169 parameter sum.\n",
      "Load vd_v2_0 with total 3746805485 parameters,206189.686 parameter sum.\n",
      "\n",
      "###################\n",
      "# Running in FP16 #\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from image2text import vd_inference\n",
    "vd_inference = vd_inference(which='v1.0', fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is [4, 768], eta 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:03<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman sitting on patio sitting on bench.\n",
      "woman sitting on her patio sitting on the beach\n",
      "a woman sitting on her balcony sitting on the sofa on the hot day.\n",
      "woman sitting on the patio sitting on the beach\n"
     ]
    }
   ],
   "source": [
    "image_path = 'assets/test_images/IMG_0364.jpeg'\n",
    "im = Image.open(image_path)\n",
    "cap = vd_inference.inference_i2t(im,20)\n",
    "\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is [4, 768], eta 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:03<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two women snow having with a friend.\n",
      "two women with their snow children riding in a snow with people.\n",
      "two women skiing for a good snow walking in their snow.\n",
      "two woman skiing on their snow\n"
     ]
    }
   ],
   "source": [
    "image_path = 'assets/test_images/ski.jpeg'\n",
    "im = Image.open(image_path)\n",
    "cap = vd_inference.inference_i2t(im,20)\n",
    "im.show()\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
