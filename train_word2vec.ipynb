{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('hcV3-stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_text = '\\n'.join(data['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9260302"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "sent_text = sent_tokenize(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = []\n",
    "for str in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\",\" \",str.lower())\n",
    "    normalized_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119863"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['concerts',\n",
       "  'are',\n",
       "  'my',\n",
       "  'most',\n",
       "  'favorite',\n",
       "  'thing',\n",
       "  'and',\n",
       "  'my',\n",
       "  'boyfriend',\n",
       "  'knew',\n",
       "  'it'],\n",
       " ['that',\n",
       "  's',\n",
       "  'why',\n",
       "  'for',\n",
       "  'our',\n",
       "  'anniversary',\n",
       "  'he',\n",
       "  'got',\n",
       "  'me',\n",
       "  'tickets',\n",
       "  'to',\n",
       "  'see',\n",
       "  'my',\n",
       "  'favorite',\n",
       "  'artist'],\n",
       " ['not',\n",
       "  'only',\n",
       "  'that',\n",
       "  'but',\n",
       "  'the',\n",
       "  'tickets',\n",
       "  'were',\n",
       "  'for',\n",
       "  'an',\n",
       "  'outdoor',\n",
       "  'show',\n",
       "  'which',\n",
       "  'i',\n",
       "  'love',\n",
       "  'much',\n",
       "  'more',\n",
       "  'than',\n",
       "  'being',\n",
       "  'in',\n",
       "  'a',\n",
       "  'crowded',\n",
       "  'stadium'],\n",
       " ['since',\n",
       "  'he',\n",
       "  'knew',\n",
       "  'i',\n",
       "  'was',\n",
       "  'such',\n",
       "  'a',\n",
       "  'big',\n",
       "  'fan',\n",
       "  'of',\n",
       "  'music',\n",
       "  'he',\n",
       "  'got',\n",
       "  'tickets',\n",
       "  'for',\n",
       "  'himself',\n",
       "  'and',\n",
       "  'even',\n",
       "  'a',\n",
       "  'couple',\n",
       "  'of',\n",
       "  'my',\n",
       "  'friends'],\n",
       " ['he',\n",
       "  'is',\n",
       "  'so',\n",
       "  'incredibly',\n",
       "  'nice',\n",
       "  'and',\n",
       "  'considerate',\n",
       "  'to',\n",
       "  'me',\n",
       "  'and',\n",
       "  'what',\n",
       "  'i',\n",
       "  'like',\n",
       "  'to',\n",
       "  'do'],\n",
       " ['i',\n",
       "  'will',\n",
       "  'always',\n",
       "  'remember',\n",
       "  'this',\n",
       "  'event',\n",
       "  'and',\n",
       "  'i',\n",
       "  'will',\n",
       "  'always',\n",
       "  'cherish',\n",
       "  'him'],\n",
       " ['on',\n",
       "  'the',\n",
       "  'day',\n",
       "  'of',\n",
       "  'the',\n",
       "  'concert',\n",
       "  'i',\n",
       "  'got',\n",
       "  'ready',\n",
       "  'and',\n",
       "  'he',\n",
       "  'picked',\n",
       "  'me',\n",
       "  'up',\n",
       "  'and',\n",
       "  'we',\n",
       "  'went',\n",
       "  'out',\n",
       "  'to',\n",
       "  'a',\n",
       "  'restaurant',\n",
       "  'beforehand'],\n",
       " ['he', 'is', 'so', 'incredibly', 'romantic'],\n",
       " ['he', 'knew', 'exactly', 'where', 'to', 'take', 'me', 'without', 'asking'],\n",
       " ['we',\n",
       "  'ate',\n",
       "  'laughed',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'wonderful',\n",
       "  'dinner',\n",
       "  'date',\n",
       "  'before',\n",
       "  'the',\n",
       "  'big',\n",
       "  'event']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences = result, vector_size = 150, window = 5, min_count = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lake', 0.8305837512016296),\n",
       " ('hiking', 0.7692774534225464),\n",
       " ('campground', 0.7646785378456116),\n",
       " ('picnic', 0.7639944553375244),\n",
       " ('sunset', 0.7639599442481995),\n",
       " ('boat', 0.7610630393028259),\n",
       " ('snorkeling', 0.7594534158706665),\n",
       " ('colorado', 0.7552204132080078),\n",
       " ('river', 0.7539743781089783),\n",
       " ('destin', 0.7503563165664673)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"beach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
